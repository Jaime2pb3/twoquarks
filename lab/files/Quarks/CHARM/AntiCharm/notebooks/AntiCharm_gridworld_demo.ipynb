{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513a1ff8",
   "metadata": {},
   "source": [
    "# Anti-CHARM v2 — Contextual Risk Demo\n",
    "\n",
    "This notebook shows a minimal usage of Anti-CHARM as a contextual\n",
    "regularizer on top of a simple tabular actor. The full environment\n", 
    "is not modeled here; the goal is only to visualize how the\n",
    "penalty and λ_t evolve over several steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "LEVO_PATH = os.path.join(ROOT, \"levo\")\n",
    "if LEVO_PATH not in sys.path:\n",
    "    sys.path.append(LEVO_PATH)\n",
    "\n",
    "from anti_charm_agent import AntiCharmAgent, AntiCharmConfig\n",
    "\n",
    "# Small synthetic tabular world\n",
    "num_states = 16\n",
    "num_actions = 4\n",
    "\n",
    "cfg = AntiCharmConfig(num_states=num_states, num_actions=num_actions)\n",
    "anti = AntiCharmAgent(cfg)\n",
    "\n",
    "def softmax(x, tau=1.0):\n",
    "    x = np.asarray(x, dtype=float)\\n",
    "    x = x / float(tau)\n",
    "    x = x - x.max()\n",
    "    ex = np.exp(x)\n",
    "    s = ex.sum()\n",
    "    return ex / max(s, 1e-8)\n",
    "\n",
    "T = 200\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "lambda_hist = []\n",
    "penalty_hist = []\n",
    "\n",
    "# Synthetic Q_charm (e.g., coming from some base actor)\n",
    "Q = rng.normal(loc=0.0, scale=1.0, size=(num_states, num_actions))\n",
    "\n",
    "diversity = 0.0\n",
    "visited_states = set()\n",
    "cum_reward = 0.0\n",
    "\n",
    "for t in range(T):\n",
    "    s = int(rng.integers(0, num_states))\n",
    "    visited_states.add(s)\n",
    "\n",
    "    # base policy from the main actor\n",
    "    p_base = softmax(Q[s], tau=1.0)\n",
    "    H_policy = -np.sum(p_base * np.log(np.clip(p_base, 1e-8, 1.0)))\n",
    "\n",
    "    # simple global stats\n",
    "    diversity = len(visited_states) / float(num_states)\n",
    "    reward_density = cum_reward / float(t + 1) if t > 0 else 0.0\n",
    "\n",
    "    stats = dict(\n",
    "        step_norm=t / float(T),\n",
    "        H_policy=H_policy,\n",
    "        temp=1.0,\n",
    "        diversity=diversity,\n",
    "        reward_density=reward_density,\n",
    "    )\n",
    "\n",
    "    p_vec, lambda_t = anti.penalty_vector(s, stats)\n",
    "\n",
    "    # effective action and synthetic reward\n",
    "    Q_eff = Q[s] - lambda_t * p_vec\n",
    "    a = int(np.argmax(Q_eff))\n",
    "    r = float(Q_eff[a])  # illustrative only\n",
    "    cum_reward += r\n",
    "\n",
    "    lambda_hist.append(lambda_t)\n",
    "    penalty_hist.append(p_vec.mean())\n",
    "\n",
    "lambda_hist = np.array(lambda_hist)\n",
    "penalty_hist = np.array(penalty_hist)\n",
    "\n",
    "print(f\"Simulated steps: {T}\")\n",
    "print(f\"Mean lambda_t: {lambda_hist.mean():.3f}\")\n",
    "print(f\"Mean penalty: {penalty_hist.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e0b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# λ_t evolution\n",
    "plt.figure()\n",
    "plt.plot(lambda_hist)\n",
    "plt.xlabel(\"Simulation step\")\n",
    "plt.ylabel(\"lambda_t\")\n",
    "plt.title(\"Anti-CHARM — λ_t evolution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a14b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean P_anti penalty\n",
    "plt.figure()\n",
    "plt.plot(penalty_hist)\n",
    "plt.xlabel(\"Simulation step\")\n",
    "plt.ylabel(\"Mean P_anti\")\n",
    "plt.title(\"Anti-CHARM — average penalty\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
